{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd00b6db1c9eb42b02ddde262e91131b34b8bc0f19661aef4215bc059af74454cb9",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "0b6db1c9eb42b02ddde262e91131b34b8bc0f19661aef4215bc059af74454cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning methods: BERT model \n",
    "\n",
    "## Jose Antonio Jijon Vorbeck\n",
    "\n",
    "In this notebook, we will apply a BERT model to the tweets data set to perform a sentiment analysis. \n",
    "\n",
    "We will compare the results obtained from this model to those obtained using classical ML models.\n",
    "\n",
    "BERT is a deep learning model that has een proven to give state-of-the art responses in natural Language Processing. BERT (Bidirectional Encoder Representations for Transformers) is a pre-trained model that has been developped by a team of Scientists at Google. It has been already trained with more than 250 million articles, including the english wikipedia and more sources.\n",
    "\n",
    "BERT relies on a Transfomer, the attention mechanism part of the algorithm. A normal Transfore system consitns of an encoder to read the input, and a decoder to output any kind of information we which to obtain from the input. But since the goal of BERT is to generate a bidirectional representation language model, we will only need the encoder part of the system. \n",
    "\n",
    "The input to the encoder from BERT needs a series of tokenized vectors, which have to be in a specific format suitable fo the algorithm. Token embedding is a crucial part of the pre-processing done for this model. The [CLS] token is embedded at the beginning of each new sentence, and the [SEP] token is used at the end of the sentences, between two consecutive sentences.\n",
    "\n",
    "We will make use of the BERT model by importing it from Tensorflow, we will be using KERAS, for a more suitable usage of the TF mechanism."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[BERT - Kaggle](https://www.kaggle.com/nayansakhiya/text-classification-using-bert)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training obs: 41156, and testing obs: 3798\n"
     ]
    }
   ],
   "source": [
    "# reading the already cleaned data\n",
    "train_data = pd.read_csv('Data/TweetC_train.csv')\n",
    "test_data = pd.read_csv('Data/TweetC_test.csv')\n",
    "print(f'Training obs: {train_data.shape[0]}, and testing obs: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping indexes with nan\n",
    "\n",
    "# training set\n",
    "index_with_nan = train_data.index[train_data.isnull().any(axis=1)]\n",
    "train_data.drop(index_with_nan,0, inplace=True)\n",
    "\n",
    "# testing set\n",
    "index_with_nan = test_data.index[test_data.isnull().any(axis=1)]\n",
    "test_data.drop(index_with_nan,0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              Corpus  label\n",
       "0  advice talk neighbour family exchange phone nu...      4\n",
       "1  coronavirus australia woolworth elderly disabl...      4\n",
       "2     food stock panic food need stay calm stay safe      4\n",
       "3  ready supermarket outbreak paranoid food stock...      0\n",
       "4  news regionâs confirmed covid case came sulli...      4"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Corpus</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>advice talk neighbour family exchange phone nu...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>coronavirus australia woolworth elderly disabl...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>food stock panic food need stay calm stay safe</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ready supermarket outbreak paranoid food stock...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>news regionâs confirmed covid case came sulli...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.iloc[:,1]\n",
    "y_test = test_data.iloc[:,1]\n",
    "\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "X_train = train_data.iloc[:,0]\n",
    "X_test = test_data.iloc[:,0]"
   ]
  },
  {
   "source": [
    "We will use the training dataset for both training and validation sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% of the training data will be devoted to the validation dataset\n",
    "train_text, validation_text, train_labels, validation_labels = train_test_split(X_train.to_numpy(), y_train.to_numpy(),test_size=0.2,random_state=0)\n",
    "\n",
    "# making the test data into numpy formats\n",
    "test_text = X_test.to_numpy()\n",
    "test_labels = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# looking for empty strings or only numbers in a string\n",
    "np.where(train_text == ' ')\n",
    "\n",
    "#x = numpy.delete(x,(2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The training data contains 32869 tweets, the validation contains 8218 tweets\n"
     ]
    }
   ],
   "source": [
    "print(f'The training data contains {len(train_text)} tweets, the validation contains {len(validation_text)} tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "source": [
    "## Since the data we are using was already cleaned, and pre-processed, we will use now it directly to train a Neural Netwrok using BERT (Bidirectional Encoder R Trasformer) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now we will aplly more specific tasks more related to the BERT algorithm, like the BERT tokenization or the BERT model itself. We will be importing these tools from the internet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT tokenization\n",
    "\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "source": [
    "Now we do more imports for SKlearn and TF, as well as the tokenization library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\jobandtalent\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\jobandtalent\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow_hub) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\jobandtalent\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow_hub) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\jobandtalent\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jobandtalent\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "source": [
    "Now we will import the BERT model from the hub.KerasLayer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing BERT Layer\n",
    "\n",
    "m_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(m_url, trainable=True)"
   ]
  },
  {
   "source": [
    "### Load the tokenizer functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "source": [
    "### Create the tokenization fuction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=160):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len-len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "source": [
    "### And the Build model function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=160):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "\n",
    "    #lay = Dense(128, activation='relu')(clf_output)\n",
    "    #lay = Dropout(0.2)(lay)\n",
    "    #lay = Dense(64, activation='relu')(lay)\n",
    "    #lay = Dropout(0.2)(lay)\n",
    "    out = Dense(5, activation='softmax')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "source": [
    "### Now we perform the text encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_text, tokenizer, max_len = 160)\n",
    "test_input = bert_encode(test_text, tokenizer, max_len = 160)\n",
    "val_input = bert_encode(validation_text, tokenizer, max_len = 160)\n",
    "\n",
    "train_labels = train_labels\n",
    "test_labels = test_labels\n",
    "val_labels = validation_labels"
   ]
  },
  {
   "source": [
    "### Build the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 160)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 160)]        0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 160)]        0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 768)          0           keras_layer[0][1]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 5)            3845        tf.__operators__.getitem[0][0]   \n==================================================================================================\nTotal params: 109,486,086\nTrainable params: 109,486,085\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len = 160)\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Applying Early Stopping and saving the best model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after every epoch.\n",
    "saveBestModel = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "# Stop training when a monitored quantity has stopped improving.\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "source": [
    "### Finally we can fit the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      " 14/658 [..............................] - ETA: 15:32:42 - loss: 0.8021 - accuracy: 0.1727 - precision: 0.1033 - recall: 0.0254 - true_positives: 11.0714"
     ]
    }
   ],
   "source": [
    "# running this in google collab because in the local server it takes much longer to complete.\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_data=(val_input, val_labels),\n",
    "    epochs=10,\n",
    "    batch_size=50,\n",
    "    callbacks=[earlyStopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}